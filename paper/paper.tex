\documentclass[sigplan]{acmart}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\usepackage[english]{babel}
\usepackage{url}
\usepackage{makecell}

\begin{document}

\title{Automatic Summarization of Scientific Texts}

\author{Maria Dobko}
\affiliation{%
  \institution{Ukrainian Catholic University\\
  Faculty of Applied Sciences}
  \city{Lviv}
  \state{Ukraine}
}
\email{dobko_m@ucu.edu.ua}

\author{Oleksandr Zaytsev}
\affiliation{%
  \institution{Ukrainian Catholic University\\
  Faculty of Applied Sciences}
  \city{Lviv}
  \state{Ukraine}
}
\email{oleks@ucu.edu.ua}

\author{Yuriy Pryima}
\affiliation{%
  \institution{Ukrainian Catholic University\\
  Faculty of Applied Sciences}
  \city{Lviv}
  \state{Ukraine}
}
\email{y.pryima@ucu.edu.ua }

\begin{abstract}

In this work we overview recent advancements in the field of abstractive text summarization and discuss the use of deep learning models, especially Generative Adversarial Networks (GAN). We apply different models to our dataset of scientifix papers, acquired from arXiv, and evaluate them in terms of simplicity and performance.

We propose our own metric based on word-embeddings, which we believe will be more suitable for evaluation of abstractive summaries.

\end{abstract}

\keywords{scientific text summarization, ROUGE, seq2seq, NLP, word embeddings}

\maketitle

\section{Introduction}

Automatic text summarization is the task of producing a concise and fluent summary while preserving key information content and overall meaning\cite{allahyari-17}.

\subsection{Abstractive vs extractive summarization}

\paragraph{Extractive text summarization} identifies and extracts important words, phrases, or sentences from a document and puts them together as a summary. As a result, the produced summary is a subset of the words from original summary\cite{kumar-16}. Most work in the field of text summarization has been done around extractive summarization. Generally, it produces better results in terms of the amount of information preserved in a short summary. 

\paragraph{Abstractive text summarization} allows us to write summaries that are almost as good as those written by humans. Generally speaking, abstractive models perform worse than extractive ones. They are hard to train and often deviate too much from the original text. And since they try to describe the meaning of a text using different words, they preserve less information. Nevertheless, abstractive summarization is closer to what humans do when they write text summaries, and for that reason this is a very promissing AI topics.

\subsection{Scientific text summarization}

The problem of summarizing scientific texts is very different from general-purpose text summarization. Every scientific paper starts with an abstract, which by its nature is the summary of a document created by its human-author. This allows us to can create a human-labeled dataset of scientific papers just by separating abstracts from the text body.

\subsection{Structure of the paper}

In this study we focus on abstractive text summarization. We do a brief overview of the related work that has been done in this field. Then we proceed to discussing different models for statistical and deep summarization. Then we present our dataset of scientific papers collected from arXiv and compare the performance of different models on this dataset. 

\section{Related work}

Das et al. made a survey of the most successful text summarization techniques as of the year 2007\cite{das-7}. Simmilar survey was made 10 years later by Allahyari et al\cite{allahyari-17}.

ROUGE - the most widely used metric for text summarization was presented by Chin-Yew Lin in July 2004\cite{lin-4}. The metric developed by us is greatly influenced by the the work of this author. Great overview of existing evaluation metrics was also done in \cite{das-7} and \cite{allahyari-17}.

Fedus, Goodfellow, and Dai\cite{fedus-18} wrote an amazing paper about their application of generative adversarial networks to the problem of filling the gaps in text. However, as we show in section \ref{sec:ml}, this approach can not be easily applied to our problem.

\section{Data collection and preparation}
\label{sec:data}

Thanks to the open policy of arXiv we were able to collect our own dataset of scientific papers from \textit{stat.ML} category\footnote{\url{https://arxiv.org/list/stat.ML/recent}}. We publish it together with this paper an encourage everyone to use it as they like in their own projects.

\paragraph{Structure of the data} Each paper on arXiv has a unique identifier (for eample: 1801.01587). It can be used to extract any data that is available and related to that paper, including its metadata and full text as PDF.

Our dataset contains 2000 documents (papers). Each one of them is represented by a row with the following properties:

\begin{enumerate}
  \item arXiv's unique identifier
  \item title
  \item abstract
  \item body of the paper
\end{enumerate}

Our script\footnote{Code and instructions for collecting the data is available in our repository: \url{https://github.com/Carouge/TextSummarization}} can be used to collect more data, but for our problem a set of 2000 papers from one category is enough. For example, DUC (Document Understanding Conference) dataset which is among the most commonly used in the feld of text summarization has 30 sets with approximately 10 documents each\footnote{\url{https://www-nlpir.nist.gov/projects/duc/guidelines/2001.html}}.

\paragraph{Cleaning the text} We collected our data as PDF files and parsed them to extract the raw text. This produced a huge amount of uniterpretable UTF-8 characters from mathematical expressions. After removing those extra characters.

We wanted to remove everething that is not a known English word, but that would filter out words like "GAN", "backprop" etc. as most standard corpuses of words, such as nltk.corpus, don't include specialized scientific terminology. So we filtered the words using manually created rules based on features like word length and frequence of vowels. This leaves some noise behind, but it will not affect our models.

\section{Models for text summarization}

Three main classes of models that are used for text summarization task are statistical frequency computation models (TFIDF etc.), graph methods (TextRank, LexRank etc.) and machine learning approach.

\subsection{Statistical methods}

These methods are based on the assumption that the importance of a word or sentence in a text depends on the total number of times it appears in the document. This means that this classical approach ignores context and lexical features of the text. Furthermore, they are able to perform only extractive summarization.

\subsection{Graph models}

We can build a graph of each document where words or sentences are nodes and the edges are the connections between each pair of nodes. The weights on these edges represent similarity between words or sentences in the whole text. While proving to have better results than simple frequency-based methods, graph models are still bounded by the absence of lexic understanding and ability to perform extractive summary only.

\subsection{Machine learning approach}
\label{sec:ml}

In the last years, lots of attention was focused on learning how to apply neural networks to NLP tasks, including text summarization. Using encoder-decoder models it is now possible to produce abstract summaries. In \cite{nallapati-16} the off-the-shelf attentional encoder-decoder RNN that was originally developed for machine translation was applied to summarization and outperformed state-of-the-art systems on two different English corpora. However, there is not much information about neural networks usage in scientific text summarization.

Generative adversarial networks (GAN) are another promising approach for text summarization. Until recent years they were considered inapplicable to the discrete problems of natural language processing (NLP). The latest papers introduce novel approaches to overcoming these issues by combining GANs with reinforcement learning.

Applying generative adversarial networks to the problems of NLP is considered to be a complicated task because GANs are only defined for continuous data, and all NLP is based on discrete values like words, characters, or bytes.

However, in their latest paper Fedus, Goodfellow, and Dai\cite{fedus-18} overcome this problem by using reinforcement learning to train the generator while the discriminator is still trained via maximum likelihood and stochastic gradient descent, and use it to fill the gaps in the text. This is an amazing result that can become an inspiration for others to develop GAN-based solutions for the problems of text sumarization. Nevertheless, summarization is way more complex than the problem of filling the gaps, and therefore, to our knowledge, there are no successful examples of applying adversarial networs to problems similar to ours. We ourselves have tried this new approach, but failed to produce good results.

\section{Evaluation metrics}
\label{sec:evaluation}

Evaluating a summary is a difficult task because there is no such thing as a single summary that would be ideal for a given document. In most cases even human evaluators can not agree on which of the given summaries is better\cite{das-7}. Unlike other NLP problems, such as translation or parsing, when it comes to text summarization we can not clearly define what makes a summary good or bad. Therefore we must make assumptions about the space of good summaries. 

\begin{enumerate}
\item assume that a good summary would be close to some \textit{ideal} summary manually created by humans
\item assume that the goodness of summary can be measured as the ammount of important information it contains (this assumption can be infered from the definition of text summarization).
\end{enumerate}

In the following sections we briefly describe some of the commonly used metrics for text summarization that is based on the first assumption and propose our own metric that is based on second one (in fact, we will show that the proposed metric can be formulated in a different way to work with the first assumption).

\subsection{ROUGE}
\label{sec:rouge}

The most widely used score for evaluating text summarizations is ROUGE (Recall-Oriented Understudy for Gisting Evaluation) inroduced by Chin-Yew Lin in 2004\cite{lin-4}.

\[ \text{ROUGE-N}(s) = \frac{\sum_{r \in R} \langle \Phi_n(r), \Phi_n(s) \rangle}{\sum_{r \in R} \langle \Phi_n(r), \Phi_n(r) \rangle} \]

\[ \text{ROUGE-L}(s) = \frac{(1 + \beta^2) R_{LCS} P_{LCS}}{R_{LCS} + \beta^2 P_{LCS}} \]

ROUGE works well for extractive text summarization. But if we need to evaluate the generated summary which contains different words from the ones that occured in paper, the score will always be small because, even though the new words can be close to the expected ones, two summaries don't overlap in terms of word equality.

For example, if the human-created summary is \textit{"The great paper"} and our model produces \textit{"A wonderful article"}, ROUGE score will be $0$, even though the summary is perfect.

\subsection{CAROUGE}

We propose a metric that uses word embeddings to evaluate summaries based on their semmantic distance to the space of good summaries. Our assumption is that a good summary of a document contains words that are semantically close to the most important words or n-grams in that document.

Let $s$ be the generated summary. If $|s|$ is the number of words in summary $s$, then $m=|s|-n+1$ is the number of $n$-grams in this summary. Let $s_1, s_2, \dots, s_m$ be all $n$-grams of summary $s$ and let $c_1, c_2, \dots, c_k$ be the $k$ most important $n$-grams in the document. As we will show in section \ref{sec:importance}, there are many ways of measuring the importance of $n$-grams in a document. The definition of $n$-gram importance is closely related to the two base assumptions that were mentioned at the beginning of section \ref{sec:evaluation}.

The metric we propose is in fact a continuous version of ROUGE-N. Instead of testing the equality of n-grams in the compared summaries we use the continuous measure of semmantic distance between those n-grams.

For each $n$-gram in the generated summary we calculate the embedding-based score as its distance to the closest important $n$-gram in the document\footnote{CAROUGE stands for Continuous Abstractive ROUGE. The French word \textbf{carouge} means blackbird}.

\[ \text{CAROUGE-N}(s_i) = \frac{1 - \operatorname*{min}_j ||s_i - c_j||}{k} \]

Now we define the score of the whole summary as the average score of its words

\[ \text{CAROUGE-N}(s) = \frac1n \sum_{i=1}^n \frac{1 - \operatorname*{min}_j ||s_i - c_j||}{k} \]

\subsubsection{Measuring word importance}
\label{sec:importance}

Deciding which words or sentences are important in a piece of text is part of the extractive summarization problem. Therefore, one way to choose $k$ important words would be to use part of a simple extractive model. For example, we could use tf-idf (term frequency-inverse document frequency) which would assign the highest scores to the words ($n$-grams) which are very frequent in the given document and very unfrequent in other documents.

Another way of choosing important words would be to follow the assumption of ROUGE, according to which a good summary should be as close as possible to the human-created summaries. This means that we will compare generated summaries to the corresponding paper abstract (which are in fact author-created summaries).

Using the same example as in the section \ref{sec:rouge} we can see that CAROUGE score of a decent abstractive summary will be greater than 0.

\[ r = \text{"The great paper"} \]
\[ s = \text{"A wonderful article"} \]
\[ \text{ROUGE}(s) = 0 \]
\[ \text{CAROUGE}(s) = 0.8956 \]

\section{Experiments and results}

We have tried different methods of text summarization, both extractive (TextRank, RAKE) and abstractive (Seq2Seq, SeqGAN).

\subsection{RAKE}

Rapid Automatic Keyword Extraction algorithm (RAKE)\cite{rose-10} is a keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text. Its main advantages include time efficiency, operating on individual documents. It can be easily applied to new domains and multiple types of documents. RAKE is very sensitive to unclean data, as it relies on word frequency.

\begin{table}[H]
\caption{Example of an abstract generated by RAKE}

\begin{center}
\begin{tabular}{|p{0.25\linewidth}|p{0.67\linewidth}|}
\hline
\textbf{Title} & The Multivariate Generalised von Mises distribution: Inference and  applications \\
\hline
\textbf{Real abstract} & ...Previously proposed multivariate circular distributions are shown to be special cases of this construction. Second, we introduce a new probabilistic model for circular regression, that is inspired by Gaussian Processes, and a method for probabilistic principal component analysis with circular hidden variables... \\
\hline
\textbf{\makecell[l]{Generated \\ abstract}} & ...many data modelling problems since higher order generalised von mises distributions model circular variables using distributional assumptions probabilistic principal component analysis ppca proposed resulting distribution inherits desirable characteristics paper makes three technical contribu tions multivariate generalised... \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{TextRank}
In this research we used a basic model TextRank as a baseline for comparison.  This is a graph method, influenced by PageRank algorithm, that represents the documents as a connected graph\cite{mihalcea-4}. We trained TextRank separately for each body of the paper and produced an extractive abstract-length summary. Scores were calculated on the basis of ROUGE score. Unlike RAKE which operates n-grams, TextRank generates summaries using whole sentences. For that reason the generated summaries are more readable.

\begin{table}[H]
\caption{Example of an abstract generated by TextRank}

\begin{center}
\begin{tabular}{|p{0.25\linewidth}|p{0.67\linewidth}|}
\hline
\textbf{Title} & Churn Prediction in Mobile Social Games: Towards a Complete Assessment Using Survival Ensembles \\
\hline
\textbf{Real abstract} & ...for each player, we predict the probability of churning as function of time, which permits to distinguish various levels of loyalty profiles ... Our results show that churn prediction by survival ensembles significantly improves the accuracy and robustness of traditional analyses, like Cox regression... \\
\hline
\textbf{\makecell[l]{Generated \\ abstract}} & Conditional inference survival ensembles are constructed based on unbiased trees avoiding this problem the resulting prediction of this model contains for each player a survival function indicating the probability of churn as a function of time since the registration in the game. \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Seq2Seq}
We have used deep LSTM seq2seq model with attention for a task of text summarization. Seq2seq have proven to provide state-of-art result in tasks of sequence generation. At this point it is too hard to produce an abstract from the text of a paper, so we started with a simpler task of generating a title from the text of an abstract
As input model takes paper abstract converted to the vectorized representation using word embeddings.  The input sequence is limited by 600 words. All abstracts that are bigger than limit are omitted. All smaller abstracts are padded with $\langle\text{SOS}\rangle$ word that represents the end of a sequence.
Model outputs sequence derived from the probability distribution. Each output word samples from this distribution having input sequence and previously generated samples. Output sequence is limited by 30 words. First $\langle\text{SOS}\rangle$ word represent the end of a generated summary.

After some training our model was able to generate meaningful titles for most abstracts. Take a look at this example:

\paragraph{Abstract} this is great popcorn and i too have the whirly pop. the unk packs work wonderfully. i have not found it too salty or the packages leak. i have found the recent price of \$35 too expensive and have purchased direct from great american for half the price.

\paragraph{Predicted summary} great popcorn!!

\paragraph{Actual summary} great unk american popcorn


\section{Evaluating results}
\label{sec:results}

In the table below you can see the results of our experiments represented by mean scores of several models that were used to generate abstracts for same 2000 papers from our dataset given their text bodies.

\begin{table}[H]
\caption{Evaluating summarization algorithms}

\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{CAROUGE-1} \\
\hline
RAKE & 0.08 & 0.02 & 0.82 \\
TextRank & 0.18 & 0.04 & 0.89 \\
\hline
\end{tabular}
\end{center}
\end{table}

We have also involved some human judges and asked them to come up with titles for 5 papers from our dataset, given their abstracts.

\begin{table}[H]
\caption{Evaluating human judges}

\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Model} & \textbf{ROUGE-1} & \textbf{ROUGE-2} & \textbf{CAROUGE-1} \\
\hline
Human & 0.77 & 0.43 & 0.98 \\
\hline
\end{tabular}
\end{center}
\end{table}

\section*{Conclusions}

In this paper we demonstrated how Generative Adversarial Networks can be used for abstractive text summarization.

As we have seen in section \ref{sec:results}, both ROUGE and CAROUGE metrics give very high scores to human-created summaries, and much lower scores for the summaries produced by algorithms. However, consider the fact that the base assumption of ROUGE takes human judgement as the ground truth

% \section*{Acknowledgements}

% We would like to express our gratitude to Anatolii Stehnii, Artem Chernodub, and Maryana Romanyshyn for helping us with their advices.

\bibliographystyle{plain}
\bibliography{paper}

\end{document}